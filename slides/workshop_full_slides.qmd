---
title: "Devops for Data Scientists"
subtitle: "wifi name:xyz wifi password:123 "
title-slide-attributes: 
  data-background-color: white
  data-background-image: _extensions/positconfslides/assets/backgrounds/toc-light.svg
  data-background-size: contain
format:
  positconfslides-revealjs: 
    chalkboard: true
    slide-number: true
    footer: <https://github.com/posit-conf-2023/devops>
    incremental: false
    code-copy: true
    center-title-slide: false
    code-link: true
    code-overflow: wrap
    highlight-style: a11y
    width: "1600"
    height: "900"
    filters:
      - positconfslides
---

## Part 1: Introductions, setup, & workshop overview

## Workshop Goals

-   To understand how devops can help you in your work as data scientists

-   To understand the main principles of Devops

-   To get hands-on experience putting code into production using common devops workflows

-   To leave the workshop with some "assets" and skills you can use in your work

## Agenda & Lab Overview {.scrollable}

+------------------------------------+------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Section/Time                       | Topics                                               | Labs                                                                                                                                                          |
+====================================+======================================================+===============================================================================================================================================================+
| Part 1                             | Workshop overview                                    | Infrastructure & wifi setup                                                                                                                                   |
|                                    |                                                      |                                                                                                                                                               |
|                                    | Logistics & setup                                    | Optional: Linux Refresher                                                                                                                                     |
|                                    |                                                      |                                                                                                                                                               |
|                                    | Introductions                                        |                                                                                                                                                               |
+------------------------------------+------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Part 2: Devops Principles          | Introduction to devops                               | Lab 1: Deploy your own Quarto website on Github Pages using GitHub Actions CI/CD                                                                              |
|                                    |                                                      |                                                                                                                                                               |
|                                    | Version control & github                             |                                                                                                                                                               |
|                                    |                                                      |                                                                                                                                                               |
|                                    | CI/CD                                                |                                                                                                                                                               |
|                                    |                                                      |                                                                                                                                                               |
|                                    | Reproducing workflows and environments               |                                                                                                                                                               |
+------------------------------------+------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Part 3: Docker for Data Scientists | How and why data scientists use docker in production | Lab #2: Write your own Dockerfile to deploy Open Source Shiny Server on [Docker playground](https://labs.play-with-docker.com/) and host an app on the server |
|                                    |                                                      |                                                                                                                                                               |
|                                    | Docker: overview and architecture                    |                                                                                                                                                               |
|                                    |                                                      |                                                                                                                                                               |
|                                    | Building docker images and containers                |                                                                                                                                                               |
|                                    |                                                      |                                                                                                                                                               |
|                                    | Ports & networking                                   |                                                                                                                                                               |
+------------------------------------+------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Part 4: Data Science in Production | Choosing your deployment method                      | Lab #3: Create an API wrapper around a trained prediction model and host it on Posit Connect                                                                  |
|                                    |                                                      |                                                                                                                                                               |
|                                    | APIs and when to use them                            |                                                                                                                                                               |
|                                    |                                                      |                                                                                                                                                               |
|                                    | Just enough auth                                     |                                                                                                                                                               |
|                                    |                                                      |                                                                                                                                                               |
|                                    | Logging & metrics & testing                          |                                                                                                                                                               |
+------------------------------------+------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Part 5: Discussion                 | Course feedback                                      | None                                                                                                                                                          |
|                                    |                                                      |                                                                                                                                                               |
|                                    | Questions for the team                               |                                                                                                                                                               |
|                                    |                                                      |                                                                                                                                                               |
|                                    | Working with other teams                             |                                                                                                                                                               |
+------------------------------------+------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+

## Supplemental Materials

-   Linux
-   vim
-   Git
-   Cheat sheets

## Most important workshop commands

-   cd
-   ls
-   pwd
-   touch
-   mkdir
-   vim
-   curl
-   echo
-   env
-   \$PATH

## What we won't cover

-   How to become a devops engineer

-   Python-based workflows

-   In-depth security and auth practices

## Meet your instructors

::: panel-tabset
## Rika

![](images/rika_bio-01.jpeg){width="25%"}

-   Solutions Engineer at Posit

-   Former Data Scientist and Data Engineer

## Andrie

## David

![](images/edavidaja.jpeg){width="25%"}

-   David Aja is a Solutions Engineer at Posit. Before joining Posit, he worked as a data scientist in the public sector.

## Gagan

![](images/gagan.png){width="25%"}

-   Gagandeep Singh is a former software engineer and data scientist who has worked in a variety of cross-technology teams before joining Posit as a Solutions Engineer.

## Sam

![](images/sam-workweek-2023-large.jpeg){width="25%"}

-   Sam Edwardes is a Solutions Engineer at Posit. He is passionate about open source software and data science. Before joining Posit Sam consulted for many different companies as a Consultant at Deloitte.
:::

## Solutions Engineering at Posit

![](images/sol-eng.png){width="25%"}

-   Posit's Solutions Engineering team aims to shrink the distance between the needs of Posit's customers and our Pro and Open Source offerings, leading with curiosity and technical excellence.

-   Our customer-facing work helps our customers deploy, install, configure, and use our Pro products.

## Special Thanks to Alex Gold!

![](images/_40A8864.jpg){width="206" height="140"}

Author of [Devops for Data Science](www.do4ds.com)

Posit Solutions Engineering Team Director

## Introduce yourself to your neighbor

Take 5 minutes and introduce yourself to your neighbors.

-   How do you think devops can help you in your work?

-   What are you most looking forward to in the conference?

## Word Cloud

## Logistics & Workshop Setup {.toc-people-dark}

## Pre-workshop Install {.brackets-light}

We encourage you to set up the following systems prior to the start of the workshop. We will also set aside time during the workshop to install and troubleshoot.

1.  Install [git](https://git-scm.com/downloads) on your laptop. You can check if git is already installed by typing `git --version` in the terminal.
2.  If you do not have a github account please create a [personal](https://docs.github.com/en/get-started/signing-up-for-github/signing-up-for-a-new-github-account) account.
3.  Make sure that you have a text editor that you are comfortable using. For example, Rstudio [IDE](https://www.rstudio.com/categories/rstudio-ide/), [vscode](https://code.visualstudio.com/download), [sublime](https://www.sublimetext.com/3).
4.  Create an account at [Docker Hub](https://hub.docker.com/).
5.  Download [Docker Desktop](https://www.docker.com/products/docker-desktop/).
6.  Download [quarto](https://quarto.org/docs/download/).
7.  Sign up for our workshop Discord channel

## Wifi & Workshop Install {.brackets-light}

Wifi Name:

Wifi: Password:

Is your wifi working? If not, let one of the instructors/TA's know as soon as possible.

## Workshop Install

Login to http://rstd.io/class, use the provided username and password and access Posit Workbench and Posit Connect.

Login to Docker Playground with your Docker Hub username and password.

## Documentation & Communication {.brackets-light}

-   All documents including slides are available at this github [repo](https://github.com/posit-conf-2023/devops)
-   We will use the Discord channel for communicating code snippets and answering questions
-   Place a note on the back of your laptop
    -   🟥 - I need help
    -   🟨 - I'm still working
    -   🟩 - I'm done

## Daily Schedule {.brackets-light}

📅 September 17 and 18, 2023\
⏰ 09:00 - 17:00\
🏨 ROOM: TBD\
✍️TBD

| Time          | Topic |
|---------------|-------|
| 10:30 - 11:00 | Break |
| 12:30 - 13:30 | Lunch |
| 15:00 - 15:30 | Break |

## Code of Conduct {.brackets-light}

https://posit.co/code-of-conduct/

## Part 2 Goals: Devops Principles

-   To understand the main principles of Devops and how they can improve data science workflows

-   To get familiar with how the `renv` package helps create reproducible environments for your R projects.

-   To get comfortable using the terminal for interacting with git and github.

-   To understand how to authenticate to github using SSH or HTTPS.

-   To practice writing and reading a yaml file.

-   To understand the basics of a quarto website.

-   To deploy a quarto site to Github Pages using Github Actions for Continuous Deployment.

## Principles and History of Devops

![](images/Screenshot%202023-08-29%20at%203.32.37%20PM.png){width="586"}

-   Collaboration

-   Continuous Integration & Delivery

-   Automation

-   Reproducibility

-   Culture change

::: notes
**Many way of defining but at its core -**

**DevOps** is a combination of cultural philosophies, practices, and tools intended to integrate the work that development and operations team do.

Let's go back to early 2000s, developers are working on creating applications and adding features, when that's done, its thrown over the fence to the ops team who is in charge of maintaining, scaling, and securing the applications -

**What are some of the problems that arise:**

-But Developers often siloed from operations teams resulting in miscommunication and inefficient processes

-monolithic environments made it difficult to quickly iterate and deploy

-all the processes are manual, opaque, error prone - explain the manual bit

-Software stacks used different languages, frameworks, databases

-conflict of interest and diff incentives - new features vs stability and security

-Ultimately it resulted in stretching release period for days or even months

**Devops for the win!**

-   Development of system of practice in the late 2000's in software and IT online communities with the goal of improving the application release process by integrating the work of the two teams

But why should we as data scientists care about this?
:::

::: footer
Illustration & Definition credit:Gitlab, https://about.gitlab.com/topics/devops/
:::

## Has this ever happened to you?

::: columns
::: {.column width="50%"}
[You come back to code from a year ago and now it doesnt run!]{.fragment fragment-index="1"}
:::

::: {.column width="50%"}
[Reproduce your environment]{.fragment fragment-index="2" style="color: blue;"}
:::
:::

::: columns
::: {.column width="50%"}
[You need to hand off your model to the Engineering team but they only code in Java]{.fragment fragment-index="3"}
:::

::: {.column width="50%"}
[Create an API]{.fragment fragment-index="4" style="color: blue;"}
:::
:::

::: columns
::: {.column width="50%"}
[Your boss asks you to share that Shiny app with a client but the ops team is too busy working on their roadmap to help you deploy it somewhere.]{.fragment fragment-index="5"}
:::

::: {.column width="50%"}
[Containerize & deploy your code]{.fragment fragment-index="6" style="color: blue;"}
:::
:::

## Why should data scientists/analysts care about devops?

::: incremental
-   Data scientists are developers!

-   Data science careers have moved from academic sphere to tech and software but education hasnt always followed

-   Automation, collaboration, testing can dramatically improve data work and improve reproducibility

-   The many hats of a data scientist

-   Improve collaboration & communication with other teams
:::

## Responsibility of the analyst

::: columns
::: {.column width="50%"}
![](images/Screenshot%202023-08-22%20at%203.26.22%20PM.png)
:::

::: {.column width="50%"}
![](images/Screenshot%202023-09-10%20at%2012.18.42%20PM.png)
:::
:::

::: notes
reproducing your results is called the "cornerstone of science" - replication crisis

2005 essay by John Ioannidis in scientific journal on crisis in scientific publishing, argument that majority medical research studies cannot be replicated.

Solo, siloed investigator limited to small sample sizes No preregistration of hypotheses being tested Post-hoc cherry picking of hypotheses with best P values Only requiring P \< .05 No replication No data sharing

**27271** Notebooks:

-   **11454** could not declare dependencies

-   **5429** could not successfully install declared dependencies

-   **9185** returned an error when ran

-   **324** returned a different result than originally reported
:::

## Devops Toolkit

-   CI/CD
-   Environment Management
-   Package Management
-   Version Control & Git workflows
-   Automated Build tools
    -   YAML
    -   Dedicated cloud & CI/CD platforms
    -   Infrastructure as Code
-   Container Orchestration
-   Continuous Monitoring and testing

::: notes
build tools for automating workflows, creating configurations for those workflows, and tools to reproduce environments consistently across workflows

discuss these very very briefly
:::

## The CI/CD Pipeline

![](images/cicd.png)

::: notes
-   an iterative cycle of small steps to quickly build, test, and deploy your code -CI/CD is a critical component tof making devops happen. -CI/CD comprises of an iterative cycle continuous integration and continuous delivery or continuous deployment. -Put together, they form a "CI/CD pipeline"---a series of automated workflows that help DevOps teams cut down on manual tasks:

-Continuous integration (CI) automatically builds, tests, and commit code changes into a shared repository; Developer writes code and commits to a repo like github. This triggers an automatic process where the code is built, tested and then either passes or fails.

-Continuous delivery (CD) delivers code changes to production-ready environments for approval; final step is sometimes manual

-Continuous deployment (CD) automatically deploys code changes to customers directly. All code based - not manual
:::

## Environment Management {.smaller}

::: notes
Content is deployed (and code is promoted) across different environments with different intended audiences.
:::

+-----------------------------------------------------------------------+--------------------------------------------------------------------------+--------------------------------+
| Dev                                                                   | Test                                                                     | Prod                           |
+=======================================================================+==========================================================================+================================+
| a place for data scientists to do exploratory analysis and experiment | as similar to prod as possible                                           | separate from dev and test     |
|                                                                       |                                                                          |                                |
| often just your local desktop                                         | code testing                                                             | created using code             |
|                                                                       |                                                                          |                                |
| data science "sandbox" with data that's as close to real as possible  | data validation                                                          | code promotion process + tests |
|                                                                       |                                                                          |                                |
| access to R/Python packages                                           | in software dev world includes integration, unit, and regression testing | completely automatic           |
+-----------------------------------------------------------------------+--------------------------------------------------------------------------+--------------------------------+

## Version Control & Workflows

![](images/Screenshot%202023-08-23%20at%2011.20.04%20AM.png)

::: notes
Version control is a main tool for Continuous Integration -

lots of variants - giit, github., svn, gitlab, etc

distributed version control - everyone has their own repo

can track changes and roll them back

can fix merge conflicts

an an iterative process to build, test, collaborate on your code to above environments. Very commonly, individuals work on separate branches that are then tested and reviewed by colleagues before they are merged into a main branch.

In addition to the action of promoting your code - its also important to have processes in place for how it happens code reviews, process for your team, how to name things, pull requests, version control with git, feature branching, automatic tests
:::

## Lab Activity

🟥 - I need help

🟨 - I'm still working

🟩 - I'm done

Login to pos.it/class with code `conftest` and create an account with your name and email

Complete Part 1 of Lab - add link

## Reproducing your environment

What are the layers that need to be reproduced across your dev, test, and prod environments?

In your day-to-day work, what's the hardest reproducibility challenge?

## Layers of reproducibility

+-------------+---------------------------------+
| Layer       | Contents                        |
+=============+=================================+
| Code        | scripts, configs, applications  |
+-------------+---------------------------------+
| Packages    | R + Python Packages             |
+-------------+---------------------------------+
| System      | R + Python Language Versions    |
|             |                                 |
|             | System Libraries                |
|             |                                 |
|             | Operating System + dependencies |
+-------------+---------------------------------+
| Hardware    | Virtual Hardware                |
|             |                                 |
|             | Physical Hardware               |
+-------------+---------------------------------+

::: notes
-   Let's say someone wanted to reproduce your project including your code and your environment. Make a list of the layers that would need to be reproduced on their machine. (For example, a layer would be the version of R that you're using)

options("repos") Run .libPaths()

-   Hints:

    -   Inspect your renv.lock file.
    -   Where are your packages being pulled from? Confirm by typing `options("repos")`.
        -   You can modify your package repository by running `options("repos" = c("<REPO-NAME>" = "https://your-repository-url.com"))` in your RStudio console.
    -   Visit the webpage where packages are being pulled from and see if you can identify package dependencies. Are packages downloaded as binaries or from source?
    -   What are your server and OS dependencies? (If you are not sure which distribution of Linux you are using, you can find it by typing `cat /etc/*-release` in your terminal)
:::

## Mechanisms for reproducibility

\*\* make this a diagram starting from least to most reproducibility

-   documenting state & version control

-   virtual environments (`renv`, `venv`) + package management

-   Containerization & docker

-   Infrastructure as Code

## Packages vs. Libraries vs. Repositories

**Package** - contains code, functions, data, and documentation.

**Library** - is a directory where packages are installed.

**Repository** - a collection of packages. CRAN is a public external repository that is a network of servers that distribute R along with R packages.

::: notes
packages - Can be be distributed as SOURCE (a directory with all package components), [BINARIES](https://solutions.posit.co/envs-pkgs/environments/repositories/index.html#binary-packages) (contains files in OS-specific format) or as a BUNDLE (compressed file containing package components, similar to source).

library - You can have user-level or project-level libraries. Run `.libPaths()` to see yours. To use a package in has to be installed in a library with `install.packages()` and then loaded into memory with `library(x)` .

repo - others include pypi, bioconducter, private repos
:::

## Renv workflow

![](images/Screenshot%202023-09-06%20at%208.31.22%20PM.png)

::: notes
1.  Use a version control system e.g.[git](https://git-scm.com/) with [GitHub](https://github.com/)

2.  One user (perhaps yourself) should explicitly initialize `renv` in the project, via [`renv::init()`](https://rstudio.github.io/renv/reference/init.html). This will create the initial `renv` lockfile, and also write the `renv` auto-loaders to the project's `.Rprofile` and `renv/activate.R`. These will ensure the right version of `renv` is downloaded and installed for your collaborators when they start in this project.

3.  Using a branching strategy push your code alongside the generated lockfile `renv.lock`. Be sure to also share the generated auto-loaders in `.Rprofile` and `renv/activate.R`.

4.  When a collaborator first launches in this project, `renv` should automatically bootstrap itself, thereby downloading and installing the appropriate version of `renv` into the project library. After this has completed, they can then use [`renv::restore()`](https://rstudio.github.io/renv/reference/restore.html) to restore the project library locally on their machine.
:::

## Example

```{R}
.libPaths()
# install.packages("renv")
renv::init()
renv::snapshot()
lapply(.libPaths(), list.files)
```

::: footer
🔍 Live code
:::

## Lab Activity

🟥 - I need help

🟨 - I'm still working

🟩 - I'm done

Complete Part 2 of Lab: Deploy Quarto with GHA including the exercises

## Build tools

::: notes
tools that automate the process of building and deploying your code once it goes from your dev env to test and prod

Tools include config files, `config` package, CI/CD software such as GHA, automatic tests
:::

![](images/Screenshot%202023-08-23%20at%202.49.21%20PM.png)

::: footer
Illustration credit:
:::

## Power of YAML

-   YAML Ain't Markup Language

-   communication of data between people and computers.

-   human friendly

-   everything is a key value pair and interpreted as maps or dictionaries

-   used for configuration files across many execution environments including Docker, virtual machines, K8s, helm, IaaS, etc

## XML {.smaller}

```{xml}
<?xml version="1.0" encoding="UTF-8" ?>
<root>
  <row>
    <name>William</name>
    <dob>1962</dob>
    <nickname>Axl Rose</nickname>
    <instruments>vocals</instruments>
    <instruments>piano</instruments>
    <last_name>Bailey</last_name>
  </row>
  <row>
    <name>Saul</name>
    <dob>1965</dob>
    <nickname>Slash</nickname>
    <instruments>guitar</instruments>
    <last_name>Hudson</last_name>
  </row>
</root>
```

## JSON {.smaller}

```{json}
[
  {
    "name": "William",
    "last name": "Bailey",
    "dob": 1962,
    "nickname": "Axl Rose",
    "instruments": [
      "vocals",
      "piano"
    ]
  },
  {
    "name": "Saul",
    "last name": "Hudson",
    "dob": 1965,
    "nickname": "Slash",
    "instruments": [
      "guitar"
    ]
  }
]
```

## YAML

```{yaml}
- name: William
  last name: Bailey
  dob: 1962
  nickname: Axl Rose
  instruments:
    - vocals
    - piano

- name: Saul
  last name: Hudson
  dob: 1965
  nickname: Slash
  instruments:
    - guitar
```

## YAML syntax

-   whitespace indentation is used to denote structure, no need for quotes nor brackets

-   Colons separate keys and their values

-   Dashes are used to denote a list

https://github.com/sd031/yaml-crash-course/blob/main/full_example.yaml

## Examples

-   https://github.com/rstudio/rstudio-docker-products/blob/dev/docker-compose.yml
-   https://github.com/Rikagx/personal-website/blob/main/.github/workflows/publish.yaml

::: footer
🔍 Live code
:::

## Lab Activity

🟥 - I need help

🟨 - I'm still working

🟩 - I'm done

Inspect your \_quarto.yml file and identify what each part of it does using the quarto site.

## Is version control secure?

-   our code is still saved locally
-   How do we make sure that the code we push to Github (or elsewhere) is secure?

## A short auth teaser

-   We can use a variety of data sharing "transfer protocols"
-   protocols specify what kind of traffic is moving between 2 machines
-   a port specifies where to direct the traffic

![](images/Screenshot%202023-09-07%20at%209.28.32%20AM.png)

::: notes
whether its email, text, files, etc A port is a virtual point where network connections start and end. Ports are software-based and managed by a computer's operating system. Ports allow computers to easily differentiate between different kinds of traffic: emails go to a different port than webpages, for instance, even though both reach a computer over the same Internet connection.
:::

## Git protocol options

| http    | https    | SSH     |
|---------|----------|---------|
| port 80 | port 443 | port 22 |

http - text sent over the internet

https - http encrypted with "SSL/TLS"

SSH - secure shell

::: notes
hypertext transfer protocol When a web user wants to load or interact with a web page, their web browser sends an HTTP request to the origin server that hosts the website's files. These requests are essentially lines of text that are sent via the internet.

https : http encrypted with SSL/TLS - digital certificates that establish an encrypted connected

SSH: secure shell, public-key cryptography to authenticate the client, used for remote logins, command line execution

HTTPS is simpler. For most services besides Github, you just have to enter in your username and password, and you'll be able to push and pull code.

You don't have to juggle multiple SSH keys around to use multiple devices.

Port 443, which HTTPS uses, is open in basically any firewall that can access the internet. That isn't always the case for SSH.

The primary downside for most people is that you must enter your Git password/token every time you push. While it gets added to a cache, it's not configured to cache permanently (though this can be changed). With SSH keys, it just uses the key file on disk every time.

Where SSH takes the lead is with the authentication factor---the key. The length of it alone makes it harder to accidentally leak, and due to it being unwieldy and unique, it's generally more secure.
:::

## Which one should I use

::: columns
::: {.column width="50%"}
https - simpler - username and password or PAT - have to either cache credentials or enter in every time. can be used with Github REST API
:::

::: {.column width="50%"}
SSH - potentially more secure - uses key for auth - created per machine once
:::
:::

## Port Examples

![](images/ports-01.jpg)

::: notes
Each port is associated with a specific process or service.

show ping localhost
:::

## Lab Activity

🟥 - I need help

🟨 - I'm still working

🟩 - I'm done

Part 3 of Lab: Deploy Quarto with GHA including the exercises

::: notes
show first part

git config --list git config --global user.name "Your Name" git config --global user.email "youremail\@yourdomain.com"
:::

## Automation Build Tools

-   GHA is one tool to automate developer workflows
-   CI/CD is just one example of these workflows
-   Runs on github servers
-   Uses yaml

![](images/Screenshot%202023-09-10%20at%201.32.42%20PM.png)

::: notes
-   A github action allows us to create workflows that are triggered by a github action such as a push or pull to a branch

-   Build tools + version control in on system

-   Parts of a GHA - trigger, job, steps

-   Workflows can include tests, markdown renders, shell scripts, cron jobs, or deployments. They can be as simple or as complicated as you need. Open-source community provides a ton of examples of actions.

-   Open source collection of "available" actions

1.  In the previous exercise we created a repo - we can use it to see what kind of actions we can create

https://github.com/Rikagx/workshop_testing/blob/master/.github/workflows/publish.yaml

github.com/actions https://github.com/r-lib/actions
:::

## Lab Activity

🟥 - I need help

🟨 - I'm still working

🟩 - I'm done

Part 4 of Lab: Deploy Quarto with GHA including the exercises

## Part 3 Goals: Docker for Data Scientists

-   to understand

## Introduction to Docker

-   Open-source tool.
-   Package applications and its dependencies in a unit called a container.
-   Create isolated environments for different experiments.
-   Share work with colleagues without environment setup issues.

::: notes
Docker is a tool that allows you to virtualize (put your computer in the cloud) everything you need to create an application or in this case a data science product You can share containers with colleagues without requiring them to have to set up their own local machines Without something like this, in order to recreate or test code that someone else wrote, you'd need every developer to download the same dependencies, configurations, scripts and make sure that it ran on their machine - whether thats a mac, or windows, or linux or some other operating system
:::

## How can docker help data scientists?

## Portability

-   diagram r code creates model/api in docker container
-   uses gha to create docker image

## Isolation

## Consistency

::: notes
-   Data scientists benefit from Docker's consistency and reproducibility.

-   Create isolated environments for different experiments.

-   Share work with colleagues without environment setup issues.

-   **Consistency:** Containers ensure that applications run the same way across different environments.

-   **Isolation:** Containers isolate applications and their dependencies, preventing conflicts.

-   **Portability:** Containers can run on any system that supports Docker, reducing "it works on my machine" issues.
:::

## Lifecycle

![](images/lifecycle.png)

::: footer
Illustration credit: Alex Gold, do4ds.com
:::

## Lifecycle Example

``` bash
docker pull postgres:12
docker pull postgres:latest
docker container ls -a
docker run -d -e POSTGRES_PASSWORD=mysecretpassword --name postgres_early imageID
docker run -d -e POSTGRES_PASSWORD=mysecretpassword --name postgres_new imageID
docker container ls -a
docker stop
docker restart
```

::: footer
🔍 Live code
:::

::: notes
a dockerfile is the recipe to build a docker images this recipe is stored in some sort of repository, this can be private, public, or dockerhub - which is the default a docker image contains lightweight instructions to create your application. Docker images use something called layers which makes it super easy and quick to update. Layers start at a base layer which is usually the linux operating system and then they go up to the application layer. a container is the environment for a running process of an image - so if an image is running then its using a container instance. You can have multiple containers running at the same time. But once you delete it everything inside of it goes away.

Let's see an example of what this looks like in practice. Lets say we need to use a postgres sql database for some testing - but we want to test using an older and a newer version of postgres.

Lets go to Dockerhub and search for it. We can see official images but there are also thousands of them created by people- hub only has images not dockerfiles or containers themselves. docker image ls - lets list all the images that we have docker pull postgres:12 - see how its pulling and extracting all these layers - but what if we want a newer version or what if we need to run both versions on our machine

docker pull postgres:latest - notice how some of these layers already exist so it takes a lot less time

Lets go on dockerhub and see if we can understand a bit more about the layers https://hub.docker.com/layers/library/postgres/12/images/sha256-a97fd76ab09599e2ddc15c90a87f9a4a2a60551d99f8e7397f12a1d606d7f0ab?context=explore -

we can see that there are a lot of layers but its hard to understand what exactly is happening - lets look at the dockerfile that shows us the recipe for postgres - this is usually saved in a [github](https://github.com/docker-library/docs/blob/master/postgres/README.md) repo - not on dockerhub which is just images

every docker file starts with a from command - this is the base layers that starts the image, then we are running some things, copying , env variables, and starting

lets run the image and see
:::

## Virtual Machine vs. Container

![](images/Screenshot%202023-09-03%20at%209.56.54%20AM.png)

::: notes
containers are very lightweight which makes it really easy and quick to spin them up

this is because the container itself doesnt have a host operating system or any hardware - intel chip, apple chip etc - this is in the docker engine or runtime which we will look at in the architecture
:::

## Docker Architecture

![](images/Screenshot%202023-08-30%20at%2011.31.56%20AM.png)

::: notes
-   Docker uses a client-server architecture. The Docker client talks to the Docker daemon.

-   The Docker client - is the primary way that users interface with Docker via CLI

-   The Docker daemon does the heavy lifting of building, running, and distributing your Docker containers

-   The Docker client and daemon can run on the same system, or you can connect a Docker client to a remote Docker daemon. The Docker client and daemon communicate using a REST API, over UNIX sockets or a network interface.

-   Registry is a Repository for Docker images (e.g., Docker Hub) where you can store and share images.

-   Docker engine is a container runtime that runs on diffrent OS's. Set up the isolated environment for your container
:::

## Mode for running containers

+------------------------------------------------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Mode                                                       | Run command       | Use case                                                                                                                                                                                                           |
+============================================================+===================+====================================================================================================================================================================================================================+
| Detached                                                   | `docker run -d`   | This runs the container in the **background** so the container keeps running until the application process exits, or you stop the container. Detached mode is often used for production purposes.                  |
+------------------------------------------------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Interactive + terminal                                     | `docker run -it`  | This runs the container in the **foreground** so you are unable to access the command prompt. Interactive mode is often used for development and testing.                                                          |
+------------------------------------------------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Remove everything once the container is done with its task | `docker run --rm` | This mode is used on foreground containers that perform **short-term tasks** such as tests or database backups. Once it is removed anything you may have downloaded or created in the container is also destroyed. |
+------------------------------------------------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

## Lab Activity

🟥 - I need help

🟨 - I'm still working

🟩 - I'm done

Complete Lab 2: Part 1

## Container Debugging

-   Interactive mode

-   `docker exec`

-   Logging

```         
docker run -it -d ubuntu
docker container ls -a 
docker exec -it CONTAINER_ID bash

docker container run -d --name mydb \
 --name mydb \
 -e MYSQL_ROOT_PASSWORD=my-secret-pw \ 
 mysql
 
 docker container logs mydb
```

::: footer
🔍 Live code
:::

## Lab Activity

🟥 - I need help

🟨 - I'm still working

🟩 - I'm done

Complete Lab 2: Part 2

## Port Mapping with `docker run -p`

```         
docker pull httpd:alpine
docker pull httpd:latest

docker inspect --format='{{.Config.ExposedPorts}}' httpd:latest
docker inspect --format='{{.Config.ExposedPorts}}' httpd:alpine

docker run -p DockerHostPort:ApplicationPort

docker run -d -p 81:80 --name httpd-latest httpd:latest
curl http://localhost:81

docker run -d -p 80:80 --name httpd-alpine httpd:alpine
curl http://localhost:80
```

::: footer
🔍 Live code
:::

## Persisting data with Docker

Stateless vs. Stateful applications volume mount bind mount external storage shared file system

## Putting it all together

https://github.com/rstudio/rstudio-docker-products/blob/dev/docker-compose.yml

::: notes
-   **docker-compose:** A tool for defining and running multi-container Docker applications.
-   Uses a YAML file to define services, networks, and volumes.
-   Simplifies the orchestration of complex applications.
:::

## Lab Activity

🟥 - I need help

🟨 - I'm still working

🟩 - I'm done

Complete Lab 2: Part 3 Complete Lab 2: Part 4

## Building Docker Images

-   Images are build using a Dockerfile or interactively "on-the-fly"

-   Steps to version and share your images on Dockerhub (or a different repository)

make this a diagram Step 1: Commit Step 2: Tag Step 3: Push

## Lab Activity

🟥 - I need help

🟨 - I'm still working

🟩 - I'm done

Complete Lab 2: Part 5

## Dockerfile Build Commands

| Command | Description |
|---------|-------------|
| FROM    |             |
| ENV     |             |
| COPY    |             |
| RUN     |             |
|         |             |
|         |             |
|         |             |

## Dockerfile Example

Walk through Posit Connect [Dockerfile](https://github.com/rstudio/rstudio-docker-products/blob/dev/connect/Dockerfile.ubuntu2204)

## Activity

Complete Lab 2: Part 6

## Part 4: Data Science in Production

## Data Science in Production {.content-dark}

-   Presentation Layer

-   Processing Layer

-   Data store Layer

## Choosing the right presentation layer

-   [Alex's flow chart](https://do4ds.com/chapters/sec1/1-2-proj-arch.html)

-   credit text

## Production "State"

Questions to ask once your content is able to be consumed by your intended audience

-   Is it reproducible?

-   Is it portable?

-   Is it maintainable?

-   Does it scale?

-   Is your code efficiently written?

-   Is it secure and accessible?

-   Can you trust your code?

## APIs can help

-   RESTful APIs

-   a way to access content by non-R users

## Structure of an API

## Activity: Create a Posit Connect API key

-   login to Connect

-   create connect key

-   use rsconnect to access

-   use curl to access

## Creating an API

-   plumber exercise

## Securing your application

-   SSL & https

-   live code

## Authentication vs. Authorization

## Deploying and hosting your API

## Activity

-   deploy api to connect

-   put shiny frontend and call api

## Testing and logging

## Activity

-   add logging and test to shiny app (if there's time)

## Activity

-   add app to server

## 
